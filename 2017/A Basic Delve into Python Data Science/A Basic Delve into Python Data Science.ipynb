{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic Delve into Python Data Science\n",
    "### *~ According to the Majestic Whims of Jay Kaiser ~*\n",
    "\n",
    "**Note**: A vast majority of the code and methodologies found here has been taken directly from [Modern NLP in Python](http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). This is merely a way for me to practice these topics myself in my own environment, using the most recent dataset as provided by Yelp. I do not claim original creation of anything found here, and all credit belongs with the authors of that Jupyter notebook. I am merely striving to learn these concepts myself in a manner that is most conductive to my own research...\n",
    "\n",
    "With that in mind, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hi. My name is Jay Kaiser, and I am in my final semester of grad school at Indiana University Bloomington, where I am receiving my M.S. in Computational Linguistics. Naturally, this has consisted mostly of classes of linguistics and of computer science separately, but occasionally a class that cleverly combines the two appears. Classes like these have slowly migrated me into the world of syntax and syntactic parsing. Moreover, such classes have taken me as well to the wonderful world of data science and data analytics, and my interest for such was only expanded upon following an internship with Kingfisher Systems, Inc. as a data scientist in Washington D.C. over the summer of 2017.\n",
    "\n",
    "However, I must continue in my studies if I ever hope to succeed. In order to rapidly acclimate myself to data science and its many aspects, I have invested in a range of resources to assist me with my learning. Nothing, though, will teach me better than merely getting my hands dirty and doing it myself, so here it goes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Yelp Dataset\n",
    "\n",
    "I have downloaded on my computer the entirety of the JSON portion of the [Yelp Dataset](https://www.yelp.com/dataset/challenge), which unzipped into six separate JSON files, each of which is outlined below.\n",
    "\n",
    "| Dataset File | Description |\n",
    "|-------------:|:------------|\n",
    "| business.json | information on each business reviewed on Yelp |\n",
    "| checkin.json | each business' hours |\n",
    "| photos.json | photos found on Yelp with their captions |\n",
    "| review.json | text reviews of each business |\n",
    "| tip.json | small quotes of advice for future visitors of the business |\n",
    "| user.json | Yelp reviewers and information describing them |\n",
    "\n",
    "However, only *business.json* and *review.json* will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: A tour of the dataset\n",
    "\n",
    "First, the files need to loaded into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"YDf95gJZaq05wvo7hTQbbQ\", \"name\": \"Richmond Town Square\", \"neighborhood\": \"\", \"address\": \"691 Richmond Rd\", \"city\": \"Richmond Heights\", \"state\": \"OH\", \"postal_code\": \"44143\", \"latitude\": 41.5417162, \"longitude\": -81.4931165, \"stars\": 2.0, \"review_count\": 17, \"is_open\": 1, \"attributes\": {\"RestaurantsPriceRange2\": 2, \"BusinessParking\": {\"garage\": false, \"street\": false, \"validated\": false, \"lot\": true, \"valet\": false}, \"BikeParking\": true, \"WheelchairAccessible\": true}, \"categories\": [\"Shopping\", \"Shopping Centers\"], \"hours\": {\"Monday\": \"10:00-21:00\", \"Tuesday\": \"10:00-21:00\", \"Friday\": \"10:00-21:00\", \"Wednesday\": \"10:00-21:00\", \"Thursday\": \"10:00-21:00\", \"Sunday\": \"11:00-18:00\", \"Saturday\": \"10:00-21:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "data_directory = os.path.join(r'C:\\Users\\jayka\\Documents', 'Datasets', 'yelp')\n",
    "business_filepath = os.path.join(data_directory, 'business.json')\n",
    "review_filepath = os.path.join(data_directory, 'review.json')\n",
    "\n",
    "with codecs.open(business_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline()\n",
    "    \n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, **text** is the field I will use. However, any one of these fields could be used as well for other types of analysis.\n",
    "\n",
    "This JSON data is not easily navigable, so it'll have to be converted into a more-usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,613 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "with codecs.open(business_filepath, encoding='utf_8') as f:\n",
    "    for business_json in f:\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "        \n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "    \n",
    "    restaurant_ids = frozenset(restaurant_ids)\n",
    "    \n",
    "    print('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 51,613 registered restaurants in the dataset, over twice as many as there had been in 2016.\n",
    "\n",
    "Separately, I'll need to find the reviews for each of the restaurants, based on their ID. This will be placed in a separate file that's easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join(r'C:\\Users\\jayka\\Documents\\Datasets', 'intermediate')\n",
    "review_text_filepath = os.path.join(intermediate_directory, 'yelp_review_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 2,929,512 restaurant reviews written to the new txt file.\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "review_count = 0\n",
    "\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(review_text_filepath, 'w', encoding='utf_8') as review_text_file:\n",
    "        with codecs.open(review_filepath, encoding='utf_8') as review_json_file:\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                review_text_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "            \n",
    "    print(u'Text from {:,} restaurant reviews written to the new text file.'.format(review_count))\n",
    "\n",
    "else:\n",
    "    with codecs.open(review_text_filepath, encoding='utf_8') as review_text_file:\n",
    "        for review_count, line in enumerate(review_text_file):\n",
    "            pass\n",
    "\n",
    "    print(u'Text from {:,} restaurant reviews written to the new txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process takes me about 2.5 minutes on my dual-core i5 laptop. Moreover, ther are a total of 2,927,731 reviews found, over three times as many as in 2016. I now have the IDs and review texts in separate files, and real parsing of the text can now begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Introduction to text processing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a means to perform NLP tasks on these reviews just collected. It is both free-to-use and incredibly efficient and fast, much moreso than NLTK. It has been designed not for research, but for commerical and industrial usage. Below, a sample review has been printed. With it, I can get a better idea of what kind of data I'm working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The staff here is great and they're nice,  wonderful and quick. People were ranting in raving about pei wei, I had to try it.  Even good yelp reviews.  I'm highly dissatisfied with the flavor of the food. This  should be labeled Asian inspired and not Asian. I've tried a variety of Chinese restaurants, this doesn't taste close to anything I've had at other Asian restaurants. Their Mongolian beef  was 5 pieces of beef and large mushrooms cut into thirds in a thick sauce. You eat the rice to wash off the nasty flavor. My shrimp was thickly coated in an overpowering  sauce as well.  I only ate some of the veggies that take center stage on a meat dish.  The center of my pork egg roll was cold. The hot N sour soup was a much thicker consistency almost like that of a chili instead of being brothy. Worst of all was the price.  This was not worth it to us. Neither me or my husband enjoyed either of  our dishes.  We didn't even eat half of our plates.  We even refused to take it home with us.  If you like and enjoy what typical Asian food tastes like,  don't waste your time here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "with codecs.open(review_text_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing this entire thing with state-of-the-art NLP technology only took 23.2 milliseconds on my computer. Super fast!\n",
    "\n",
    "For listing all of the sentences in the sample review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "The staff here is great and they're nice,  wonderful and quick.\n",
      "\n",
      "Sentence 2:\n",
      "People were ranting in raving about pei wei, I had to try it.  \n",
      "\n",
      "Sentence 3:\n",
      "Even good yelp reviews.  \n",
      "\n",
      "Sentence 4:\n",
      "I'm highly dissatisfied with the flavor of the food.\n",
      "\n",
      "Sentence 5:\n",
      "This  should be labeled Asian inspired and not Asian.\n",
      "\n",
      "Sentence 6:\n",
      "I've tried a variety of Chinese restaurants, this doesn't taste close to anything I've had at other Asian restaurants.\n",
      "\n",
      "Sentence 7:\n",
      "Their Mongolian beef  was 5 pieces of beef and large mushrooms cut into thirds in a thick sauce.\n",
      "\n",
      "Sentence 8:\n",
      "You eat the rice to wash off the nasty flavor.\n",
      "\n",
      "Sentence 9:\n",
      "My shrimp was thickly coated in an overpowering  sauce as well.  \n",
      "\n",
      "Sentence 10:\n",
      "I only ate some of the veggies that take center stage on a meat dish.  \n",
      "\n",
      "Sentence 11:\n",
      "The center of my pork egg roll was cold.\n",
      "\n",
      "Sentence 12:\n",
      "The hot N sour soup was a much thicker consistency almost like that of a chili instead of being brothy.\n",
      "\n",
      "Sentence 13:\n",
      "Worst of all was the price.  \n",
      "\n",
      "Sentence 14:\n",
      "This was not worth it to us.\n",
      "\n",
      "Sentence 15:\n",
      "Neither me or my husband enjoyed either of  our dishes.  \n",
      "\n",
      "Sentence 16:\n",
      "We didn't even eat half of our plates.  \n",
      "\n",
      "Sentence 17:\n",
      "We even refused to take it home with us.  \n",
      "\n",
      "Sentence 18:\n",
      "If you like and enjoy what typical Asian food tastes like,  don't waste your time here.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing all of the named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Asian - NORP\n",
      "\n",
      "Entity 2: Asian - NORP\n",
      "\n",
      "Entity 3: Chinese - NORP\n",
      "\n",
      "Entity 4: Asian - NORP\n",
      "\n",
      "Entity 5: Mongolian - NORP\n",
      "\n",
      "Entity 6: 5 - CARDINAL\n",
      "\n",
      "Entity 7: half - CARDINAL\n",
      "\n",
      "Entity 8: Asian - NORP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing each word's POS tag:\n",
    "\n",
    "(We'll be using *pandas* for this for its easy displaying and further parsing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text part_of_speech\n",
       "0          The            DET\n",
       "1        staff           NOUN\n",
       "2         here            ADV\n",
       "3           is           VERB\n",
       "4        great            ADJ\n",
       "5          and          CCONJ\n",
       "6         they           PRON\n",
       "7          're           VERB\n",
       "8         nice            ADJ\n",
       "9            ,          PUNCT\n",
       "10                      SPACE\n",
       "11   wonderful            ADJ\n",
       "12         and          CCONJ\n",
       "13       quick            ADJ\n",
       "14           .          PUNCT\n",
       "15      People           NOUN\n",
       "16        were           VERB\n",
       "17     ranting           VERB\n",
       "18          in            ADP\n",
       "19      raving           VERB\n",
       "20       about            ADP\n",
       "21         pei          PROPN\n",
       "22         wei          PROPN\n",
       "23           ,          PUNCT\n",
       "24           I           PRON\n",
       "25         had           VERB\n",
       "26          to           PART\n",
       "27         try           VERB\n",
       "28          it           PRON\n",
       "29           .          PUNCT\n",
       "..         ...            ...\n",
       "218    refused           VERB\n",
       "219         to           PART\n",
       "220       take           VERB\n",
       "221         it           PRON\n",
       "222       home            ADV\n",
       "223       with            ADP\n",
       "224         us           PRON\n",
       "225          .          PUNCT\n",
       "226                     SPACE\n",
       "227         If            ADP\n",
       "228        you           PRON\n",
       "229       like           VERB\n",
       "230        and          CCONJ\n",
       "231      enjoy           VERB\n",
       "232       what           NOUN\n",
       "233    typical            ADJ\n",
       "234      Asian            ADJ\n",
       "235       food           NOUN\n",
       "236     tastes           NOUN\n",
       "237       like            ADP\n",
       "238          ,          PUNCT\n",
       "239                     SPACE\n",
       "240         do           VERB\n",
       "241        n't            ADV\n",
       "242      waste           VERB\n",
       "243       your            ADJ\n",
       "244       time           NOUN\n",
       "245       here            ADV\n",
       "246          .          PUNCT\n",
       "247         \\n          SPACE\n",
       "\n",
       "[248 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lemmas and shape analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>staff</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td>be</td>\n",
       "      <td>'xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td>quick</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td>people</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td>be</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td>rant</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td>rave</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td>pei</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td>wei</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td>have</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td>try</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td>refuse</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td>if</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td>what</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td>typical</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>asian</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td>taste</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td>not</td>\n",
       "      <td>x'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td>waste</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_shape\n",
       "0          The         the         Xxx\n",
       "1        staff       staff        xxxx\n",
       "2         here        here        xxxx\n",
       "3           is          be          xx\n",
       "4        great       great        xxxx\n",
       "5          and         and         xxx\n",
       "6         they      -PRON-        xxxx\n",
       "7          're          be         'xx\n",
       "8         nice        nice        xxxx\n",
       "9            ,           ,           ,\n",
       "10                                    \n",
       "11   wonderful   wonderful        xxxx\n",
       "12         and         and         xxx\n",
       "13       quick       quick        xxxx\n",
       "14           .           .           .\n",
       "15      People      people       Xxxxx\n",
       "16        were          be        xxxx\n",
       "17     ranting        rant        xxxx\n",
       "18          in          in          xx\n",
       "19      raving        rave        xxxx\n",
       "20       about       about        xxxx\n",
       "21         pei         pei         xxx\n",
       "22         wei         wei         xxx\n",
       "23           ,           ,           ,\n",
       "24           I      -PRON-           X\n",
       "25         had        have         xxx\n",
       "26          to          to          xx\n",
       "27         try         try         xxx\n",
       "28          it      -PRON-          xx\n",
       "29           .           .           .\n",
       "..         ...         ...         ...\n",
       "218    refused      refuse        xxxx\n",
       "219         to          to          xx\n",
       "220       take        take        xxxx\n",
       "221         it      -PRON-          xx\n",
       "222       home        home        xxxx\n",
       "223       with        with        xxxx\n",
       "224         us      -PRON-          xx\n",
       "225          .           .           .\n",
       "226                                   \n",
       "227         If          if          Xx\n",
       "228        you      -PRON-         xxx\n",
       "229       like        like        xxxx\n",
       "230        and         and         xxx\n",
       "231      enjoy       enjoy        xxxx\n",
       "232       what        what        xxxx\n",
       "233    typical     typical        xxxx\n",
       "234      Asian       asian       Xxxxx\n",
       "235       food        food        xxxx\n",
       "236     tastes       taste        xxxx\n",
       "237       like        like        xxxx\n",
       "238          ,           ,           ,\n",
       "239                                   \n",
       "240         do          do          xx\n",
       "241        n't         not         x'x\n",
       "242      waste       waste        xxxx\n",
       "243       your      -PRON-        xxxx\n",
       "244       time        time        xxxx\n",
       "245       here        here        xxxx\n",
       "246          .           .           .\n",
       "247         \\n          \\n          \\n\n",
       "\n",
       "[248 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "            columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each lemma is its token, minus inflections or tenses. Also note that all pronouns have been converted to *-PRON-*. (I can only guess this is because all pronouns can be see as identical information-wise, so there's no reason to save them as separate lemmas.)\n",
    "\n",
    "For listing tokens and their analytics (like what kind of named entity they are and where a token lies within a named entity phrase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>NORP</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "0          The                                O\n",
       "1        staff                                O\n",
       "2         here                                O\n",
       "3           is                                O\n",
       "4        great                                O\n",
       "5          and                                O\n",
       "6         they                                O\n",
       "7          're                                O\n",
       "8         nice                                O\n",
       "9            ,                                O\n",
       "10                                            O\n",
       "11   wonderful                                O\n",
       "12         and                                O\n",
       "13       quick                                O\n",
       "14           .                                O\n",
       "15      People                                O\n",
       "16        were                                O\n",
       "17     ranting                                O\n",
       "18          in                                O\n",
       "19      raving                                O\n",
       "20       about                                O\n",
       "21         pei                                O\n",
       "22         wei                                O\n",
       "23           ,                                O\n",
       "24           I                                O\n",
       "25         had                                O\n",
       "26          to                                O\n",
       "27         try                                O\n",
       "28          it                                O\n",
       "29           .                                O\n",
       "..         ...         ...                  ...\n",
       "218    refused                                O\n",
       "219         to                                O\n",
       "220       take                                O\n",
       "221         it                                O\n",
       "222       home                                O\n",
       "223       with                                O\n",
       "224         us                                O\n",
       "225          .                                O\n",
       "226                                           O\n",
       "227         If                                O\n",
       "228        you                                O\n",
       "229       like                                O\n",
       "230        and                                O\n",
       "231      enjoy                                O\n",
       "232       what                                O\n",
       "233    typical                                O\n",
       "234      Asian        NORP                    B\n",
       "235       food                                O\n",
       "236     tastes                                O\n",
       "237       like                                O\n",
       "238          ,                                O\n",
       "239                                           O\n",
       "240         do                                O\n",
       "241        n't                                O\n",
       "242      waste                                O\n",
       "243       your                                O\n",
       "244       time                                O\n",
       "245       here                                O\n",
       "246          .                                O\n",
       "247         \\n                                O\n",
       "\n",
       "[248 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "            columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing stopwords, punctuation, whitespace, numbers, unstandard vocab words, probability of appearance in the corpus, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>-5.774222</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>-10.720455</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>-7.175437</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>-4.329765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td>-7.822114</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>-4.195279</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>-5.429816</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td>-6.377125</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>-8.462502</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>-4.612954</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>-10.470794</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>-4.195279</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td>-9.907201</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td>-8.621843</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td>-6.634815</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>-4.585874</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td>-5.880750</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td>-4.064181</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td>-6.679681</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td>-7.992982</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>-4.506450</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td>-10.914844</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td>-7.310444</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td>-4.506450</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td>-8.329909</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td>-7.434524</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td>-4.612954</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td>-6.505217</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td>-4.547973</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td>-5.808349</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td>-4.195279</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>-9.455478</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td>-5.972965</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td>-10.238110</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>-11.071413</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td>-8.707296</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td>-11.151585</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td>-5.808349</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td>-4.612954</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td>-5.300070</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td>-4.883962</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td>-9.453293</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td>-5.868531</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td>-6.559485</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td>-7.175437</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-6.179879</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0          The        -5.774222   Yes                                    \n",
       "1        staff       -10.720455                                          \n",
       "2         here        -7.175437   Yes                                    \n",
       "3           is        -4.329765   Yes                                    \n",
       "4        great        -7.822114                                          \n",
       "5          and        -4.195279   Yes                                    \n",
       "6         they        -5.429816   Yes                                    \n",
       "7          're        -6.377125                                          \n",
       "8         nice        -8.462502                                          \n",
       "9            ,        -3.391480                Yes                       \n",
       "10                    -4.612954                            Yes           \n",
       "11   wonderful       -10.470794                                          \n",
       "12         and        -4.195279   Yes                                    \n",
       "13       quick        -9.907201                                          \n",
       "14           .        -3.072948                Yes                       \n",
       "15      People        -8.621843                                          \n",
       "16        were        -6.634815   Yes                                    \n",
       "17     ranting       -19.579313                                          \n",
       "18          in        -4.585874   Yes                                    \n",
       "19      raving       -19.579313                                          \n",
       "20       about        -5.880750   Yes                                    \n",
       "21         pei       -19.579313                                          \n",
       "22         wei       -19.579313                                          \n",
       "23           ,        -3.391480                Yes                       \n",
       "24           I        -4.064181   Yes                                    \n",
       "25         had        -6.679681   Yes                                    \n",
       "26          to        -3.838520   Yes                                    \n",
       "27         try        -7.992982                                          \n",
       "28          it        -4.506450   Yes                                    \n",
       "29           .        -3.072948                Yes                       \n",
       "..         ...              ...   ...          ...         ...     ...   \n",
       "218    refused       -10.914844                                          \n",
       "219         to        -3.838520   Yes                                    \n",
       "220       take        -7.310444   Yes                                    \n",
       "221         it        -4.506450   Yes                                    \n",
       "222       home        -8.329909                                          \n",
       "223       with        -5.363765   Yes                                    \n",
       "224         us        -7.434524   Yes                                    \n",
       "225          .        -3.072948                Yes                       \n",
       "226                   -4.612954                            Yes           \n",
       "227         If        -6.505217   Yes                                    \n",
       "228        you        -4.547973   Yes                                    \n",
       "229       like        -5.808349                                          \n",
       "230        and        -4.195279   Yes                                    \n",
       "231      enjoy        -9.455478                                          \n",
       "232       what        -5.972965   Yes                                    \n",
       "233    typical       -10.238110                                          \n",
       "234      Asian       -11.071413                                          \n",
       "235       food        -8.707296                                          \n",
       "236     tastes       -11.151585                                          \n",
       "237       like        -5.808349                                          \n",
       "238          ,        -3.391480                Yes                       \n",
       "239                   -4.612954                            Yes           \n",
       "240         do        -5.300070   Yes                                    \n",
       "241        n't        -4.883962                                          \n",
       "242      waste        -9.453293                                          \n",
       "243       your        -5.868531   Yes                                    \n",
       "244       time        -6.559485                                          \n",
       "245       here        -7.175437   Yes                                    \n",
       "246          .        -3.072948                Yes                       \n",
       "247         \\n        -6.179879                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   \n",
       "5                   \n",
       "6                   \n",
       "7                   \n",
       "8                   \n",
       "9                   \n",
       "10                  \n",
       "11                  \n",
       "12                  \n",
       "13                  \n",
       "14                  \n",
       "15                  \n",
       "16                  \n",
       "17                  \n",
       "18                  \n",
       "19                  \n",
       "20                  \n",
       "21                  \n",
       "22                  \n",
       "23                  \n",
       "24                  \n",
       "25                  \n",
       "26                  \n",
       "27                  \n",
       "28                  \n",
       "29                  \n",
       "..             ...  \n",
       "218                 \n",
       "219                 \n",
       "220                 \n",
       "221                 \n",
       "222                 \n",
       "223                 \n",
       "224                 \n",
       "225                 \n",
       "226                 \n",
       "227                 \n",
       "228                 \n",
       "229                 \n",
       "230                 \n",
       "231                 \n",
       "232                 \n",
       "233                 \n",
       "234                 \n",
       "235                 \n",
       "236                 \n",
       "237                 \n",
       "238                 \n",
       "239                 \n",
       "240                 \n",
       "241                 \n",
       "242                 \n",
       "243                 \n",
       "244                 \n",
       "245                 \n",
       "246                 \n",
       "247                 \n",
       "\n",
       "[248 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                    token.prob,\n",
    "                    token.is_stop,\n",
    "                    token.is_punct,\n",
    "                    token.is_space,\n",
    "                    token.like_num,\n",
    "                    token.is_oov)\n",
    "                   for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                 columns=['text',\n",
    "                         'log_probability',\n",
    "                         'stop?',\n",
    "                         'punctuation?',\n",
    "                         'whitespace?',\n",
    "                         'number?',\n",
    "                         'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                      .applymap(lambda x: u'Yes' if x else u''))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Automatic Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with these tokens is that independently, they do not properly reflect the relationships between one another. For example, above, *do* and *n't* are considered separate tokens, but they would be better reflected as a single collocation since *n't* will never appear apart from *do* (or alternative modal verbs).\n",
    "\n",
    "From the *gensim* library, I will be able to complete phrase modeling over the current list of tokens to find such relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Because of Windows Anaconda, a warning appears without this filter.\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four steps that must be done in data preparation here:\n",
    "- a) Segment text of complete reviews into sentences and normalize text\n",
    "- b) First-order phrase modeling\n",
    "- c) Second-order phrase modeling\n",
    "- d) Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "#### a) Segment text of complete reviews into sentences & normalize text\n",
    "\n",
    "Three helper functions will be used for this step. They will, respectively, find punctuation and whitespace, line breaks, and word lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\" eliminates purely punctuation and whitespace tokens\"\"\"\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"reads in each review and unescapes the line-breaks\"\"\"\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"uses spaCy to parse reviews, lemmatize, and tokenize sentences\"\"\"\n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                 batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                            if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*lemmatized_sentence_corpus* loops over the original review text and outputs it to *unigram_sentences_all* one normalized sentence at a time, with one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_text_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process took me 2 hours and 48 minutes to complete on my modest computer.\n",
    "\n",
    "The Gensim library provides a LineSentence class that streams documents of this format from disk, allowing massive scaling for huge amounts of data that now does not need to be saved in whole into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these sentences look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this place be clean the staff be wonderful and go out of -PRON- way to help -PRON-\n",
      "\n",
      "-PRON- stay a step ahead of -PRON- and get what what -PRON- ne the food be perfect\n",
      "\n",
      "-PRON- have eat here many many time and -PRON- have always be fresh hot and delicious -PRON- highly recommend this place 100%\n",
      "\n",
      "service be a 4\n",
      "\n",
      "very sloppy also the hot and sour soup be not that bad if u add enough chile paste and salt and soy sauce the gentleman name pop -PRON- be the manager great service from -PRON- and this other lady do not get -PRON- name unfortunetly -PRON- have change -PRON- rating 2 time that be how impressed -PRON- be\n",
      "\n",
      "love the food but -PRON- be great at all location\n",
      "\n",
      "the customer service be horrible at this location\n",
      "\n",
      "only two table be occupy at the time -PRON- walk in for a carry out\n",
      "\n",
      "-PRON- walk in and -PRON- look at -PRON- like really\n",
      "\n",
      "go away\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 450, 460):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these new sentences are far simpler grammatically than they would have been before. This presents a glimpse at what the final outcome of this stage will look like in the end.\n",
    "\n",
    "#### b) First-order phrase modeling\n",
    "\n",
    "From these normalized sentences, one can now use *gensim.models.Phrases* to find two-word collocations and link them together into single tokens. \n",
    "\n",
    "For example, in one of the sentences above are the words \"soy sauce\". Though they are technically separate words, when together as a bigram they act as a distinct entity. Hence, by training a model on frequent bigrams, one can disambiguate cases like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This process took a little more than 14 minutes to complete on my computer. By saving the model to its filepath, it can be run on unrelated text. This process allows different models formed from different corpora to be cross-compared.\n",
    "\n",
    "I can now edit the original sentences to account for these bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                        'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a little more than 26 minutes on my computer. Let's take a look at what I'm left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing fantastic about -PRON- but -PRON- taste fine\n",
      "\n",
      "-PRON- come with a sweet mustard dip sauce which be pretty tasty\n",
      "\n",
      "usual filling of pork veggie etc\n",
      "\n",
      "deep_fried pot_stickers- -PRON- get 2 of these thing\n",
      "\n",
      "-PRON- be good\n",
      "\n",
      "small but good\n",
      "\n",
      "straight pork filling in the middle of -PRON-\n",
      "\n",
      "overall -PRON- be just alright\n",
      "\n",
      "-PRON- will say that -PRON- give -PRON- a grip of food\n",
      "\n",
      "the 2 entree -PRON- get could have easily_feed 1 2 more people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what has been achieved? Looking at the outputted sentences above, now two-word phrases like \"deep fried\" and \"pot stickers\" are now written out as \"deep\\_fried\" and \"pot\\_stickers\". This gives a better representation of what words co-occur together at a rate far greater than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Second-order phrase modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have concatenated bigrams, but having trigrams accounted for as well might be a good idea. This involves nearly the same exact code found in part (b), but this time instead of running the new trigram model over the unigram sentences, I'll run it over the just-made bigram ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                     'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:  # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process took almost 18 minutes to complete on my computer. Again, I'm saving this trigram model in its own file so that it could be used again in the future on other corpora. Now, this second-order phrase model can be applied to the first-order transformed sentences, and the results of such will be outputted to a third sentences file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:  # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took almost 27 minutes on my computer. Now there is a file consisting of every sentence marked with trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitely a second good to p.f._chang_'s\n",
      "\n",
      "service be fast and on time\n",
      "\n",
      "love the online_ordering very convenient food be always hot and delicious\n",
      "\n",
      "however\n",
      "\n",
      "no forks for take out\n",
      "\n",
      "that be poor management\n",
      "\n",
      "either -PRON- do not order in time or there be a problem with the order either way hustle -PRON- booty to walmart and buy a couple box unacceptable\n",
      "\n",
      "the food be fantastic -PRON- highly_recommend the lettuce_wraps\n",
      "\n",
      "then -PRON- go to dinner at pei_wei\n",
      "\n",
      "not bad for an asian_fusion place fill with senior_citizen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 270, 280):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I do all this? Because now, instead of the phrase \"p.f. chang 's\" being represented as three separate words, it is now concatenated into \"p.f.\\_chang\\_'s\". This is now correctly marked as a single entity instead of as three.\n",
    "\n",
    "Technically, I could repeat this process indefinitely with higher and higher ngrams, but very little is gained information-wise after trigrams, so here I stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Apply text normalization and second-order phrase model to text of complete reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? I now have these useful models and lots of sentences, but I was originally looking at full reviews. Therefore, I can run the original texts through these models (and even remove imformation-poor stop-words to boot) and be left with a semantically-rich equivalent to our original text, ignoring most of grammar and focusing on the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                       'trigram_tranformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 47min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_review in nlp.pipe(line_review(review_text_filepath),\n",
    "                                    batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize, and remove punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "            \n",
    "            # apply the first- and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove stopwords, using SpaCy's nifty premade list\n",
    "            trigram_review = [term for term in trigram_review if term not in spacy.en.STOP_WORDS]\n",
    "            \n",
    "            # finally, output this to the new file, review by review as line by line\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a little more than 3 hours and 47 minutes to complete on my computer.\n",
    "\n",
    "Thus, to compare the text from before and after NLP transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "I love this place i'd recommend it to anyone ! We always order it togo and it never disappoints! The food always taste fresh and is always ready on time! Definitely our favorite lunch spot !\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "-PRON- love place -PRON- recommend -PRON- -PRON- order -PRON- togo -PRON- disappoint food taste fresh ready time definitely -PRON- favorite lunch spot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'Original:\\n')\n",
    "for review in it.islice(line_review(review_text_filepath), 11, 12):\n",
    "    print(review)\n",
    "    \n",
    "print(u'----\\n')\n",
    "print(u'Transformed:\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious through a comparison of both of these that while a lot of content has been removed from the original text, the meat-and-potatoes remains. This information is far more useful for the semantic topic modelling that'll be applied next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualizing topic models with pyLDAvis\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bb1798437e3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamulticore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Because of Windows Anaconda, a warning appears without this filter.\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram-dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if True: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "    \n",
    "    # iterate the reviews and build a dictionary of trigrams\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are too rare or common (so little information gain) from the dict and reassign integer ids\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "    \n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "\n",
    "# load the finished dictionary back in after it's been built\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                   'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function that reads in a file and yields a bow representation from it\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if True: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    #generate the bow representation for the reviews and save as a matrix\n",
    "    MmCorpus.serialize(trigram_bow-filepath,\n",
    "                      trigram_bow_generator(trigram_reviews_filepath))\n",
    "\n",
    "# load the finished corpus back in after it's been built\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory,\n",
    "                                 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if True: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                          num_topics=50,\n",
    "                          id2word=trigram_dictionary,\n",
    "                          workers=1)\n",
    "        \n",
    "    lda.save(lda_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    prints out a formatted list of the top terms of a given topic number\n",
    "    \"\"\"\n",
    "    \n",
    "    print(u'{:20} {}\\n'.format(u'term', u'frequency'))\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore_topic(topic_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0: u'',\n",
    "              1: u'',\n",
    "              2: u'',\n",
    "              3: u'',\n",
    "              4: u'',\n",
    "              5: u'',\n",
    "              6: u'',\n",
    "              7: u'',\n",
    "              8: u'',\n",
    "              9: u'',\n",
    "              10: u'',\n",
    "              11: u'',\n",
    "              12: u'',\n",
    "              13: u'',\n",
    "              14: u'',\n",
    "              15: u'',\n",
    "              16: u'',\n",
    "              17: u'',\n",
    "              18: u'',\n",
    "              19: u'',\n",
    "              20: u'',\n",
    "              21: u'',\n",
    "              22: u'',\n",
    "              23: u'',\n",
    "              24: u'',\n",
    "              25: u'',\n",
    "              26: u'',\n",
    "              27: u'',\n",
    "              28: u'',\n",
    "              29: u'',\n",
    "              30: u'',\n",
    "              31: u'',\n",
    "              32: u'',\n",
    "              33: u'',\n",
    "              34: u'',\n",
    "              35: u'',\n",
    "              36: u'',\n",
    "              37: u'',\n",
    "              38: u'',\n",
    "              39: u'',\n",
    "              40: u'',\n",
    "              41: u'',\n",
    "              42: u'',\n",
    "              43: u'',\n",
    "              44: u'',\n",
    "              45: u'',\n",
    "              46: u'',\n",
    "              47: u'',\n",
    "              48: u'',\n",
    "              49: u''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory,\n",
    "                                   'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'w') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory,\n",
    "                                   'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if True:\n",
    "    \n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda,\n",
    "                                              trigram_bow_corpus,\n",
    "                                              trigram_dictionary)\n",
    "    \n",
    "    with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the finished pyLDAvis data back in after it's been built       \n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_text_filepath),\n",
    "                         review_number,\n",
    "                         review_number + 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    take the original text of a review and\n",
    "    (1) parse it using SpaCy\n",
    "    (2) pre-process the text\n",
    "    (3) create a bow representation\n",
    "    (4) create an LDA representation\n",
    "    (5) print a sorted list of the top topics from the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # (1) parse the text using SpaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # (2) pre-process the text\n",
    "    #  - lemmatize and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                     if not punct_space(token)]\n",
    "    \n",
    "    #  - apply the first- and second-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    #  - remove stopwords\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                     if not term in spacy.en.STOP_WORDS]\n",
    "    \n",
    "    # (3) create a bow representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # (4) create an LDA representation\n",
    "    review_lda = sorted(review_lda, key=lambda(topic_number, freq): -freq)\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "        \n",
    "        # (5) print a sorted list of the top topics from the LDA representation\n",
    "        print('{:25} {}'.format(topic_names[topic_number],\n",
    "                               round(freq, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(50)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(100)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Word vector models with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualizing word2vec with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
