{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic Delve into Python NLP\n",
    "### *~ According to the Majestic Whims of Jay Kaiser ~*\n",
    "\n",
    "**Note**: A vast majority of the code and methodologies found here has been taken directly from [Modern NLP in Python](http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). This is merely a way for me to practice these topics myself in my own environment, using the most recent dataset as provided by Yelp. I do not claim original creation of anything found here, and all credit belongs with the authors of that Jupyter notebook. I am merely striving to learn these concepts myself in a manner that is most conductive to my own research...\n",
    "\n",
    "With that in mind, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hi. My name is Jay Kaiser, and I am in my final semester of grad school at Indiana University Bloomington, where I am receiving my M.S. in Computational Linguistics. Naturally, this has consisted mostly of classes of linguistics and of computer science separately, but occasionally a class that cleverly combines the two appears. Classes like these have slowly migrated me into the world of syntax and syntactic parsing. Moreover, such classes have taken me as well to the wonderful world of data science and data analytics, and my interest for such was only expanded upon following an internship with Kingfisher Systems, Inc. as a data scientist in Washington D.C. over the summer of 2017.\n",
    "\n",
    "However, I must continue in my studies if I ever hope to succeed. In order to rapidly acclimate myself to data science and its many aspects, I have invested in a range of resources to assist me with my learning. Nothing, though, will teach me better than merely getting my hands dirty and doing it myself, so here it goes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Yelp Dataset\n",
    "\n",
    "I have downloaded on my computer the entirety of the JSON portion of the [Yelp Dataset](https://www.yelp.com/dataset/challenge), which unzipped into six separate JSON files, each of which is outlined below.\n",
    "\n",
    "| Dataset File | Description |\n",
    "|-------------:|:------------|\n",
    "| business.json | information on each business reviewed on Yelp |\n",
    "| checkin.json | each business' hours |\n",
    "| photos.json | photos found on Yelp with their captions |\n",
    "| review.json | text reviews of each business |\n",
    "| tip.json | small quotes of advice for future visitors of the business |\n",
    "| user.json | Yelp reviewers and information describing them |\n",
    "\n",
    "However, only *business.json* and *review.json* will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: A tour of the dataset\n",
    "\n",
    "First, the files need to loaded into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jayka\\\\Documents\\\\Datasets\\\\yelp\\\\business.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25e1060d5b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mreview_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'review.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbusiness_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf_8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mfirst_business_record\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jayka\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[1;31m# Force opening of the file in binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jayka\\\\Documents\\\\Datasets\\\\yelp\\\\business.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "data_directory = os.path.join(r'C:\\Users\\jayka\\Documents', 'Datasets', 'yelp')\n",
    "business_filepath = os.path.join(data_directory, 'business.json')\n",
    "review_filepath = os.path.join(data_directory, 'review.json')\n",
    "\n",
    "with codecs.open(business_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline()\n",
    "    \n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, **text** is the field I will use. However, any one of these fields could be used as well for other types of analysis.\n",
    "\n",
    "This JSON data is not easily navigable, so it'll have to be converted into a more-usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "with codecs.open(business_filepath, encoding='utf_8') as f:\n",
    "    for business_json in f:\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "        \n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "    \n",
    "    restaurant_ids = frozenset(restaurant_ids)\n",
    "    \n",
    "    print('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 51,613 registered restaurants in the dataset, over twice as many as there had been in 2016.\n",
    "\n",
    "Separately, I'll need to find the reviews for each of the restaurants, based on their ID. This will be placed in a separate file that's easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join(r'C:\\Users\\jayka\\Documents\\Datasets', 'intermediate')\n",
    "review_text_filepath = os.path.join(intermediate_directory, 'yelp_review_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "review_count = 0\n",
    "\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(review_text_filepath, 'w', encoding='utf_8') as review_text_file:\n",
    "        with codecs.open(review_filepath, encoding='utf_8') as review_json_file:\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                review_text_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "            \n",
    "    print(u'Text from {:,} restaurant reviews written to the new text file.'.format(review_count))\n",
    "\n",
    "else:\n",
    "    with codecs.open(review_text_filepath, encoding='utf_8') as review_text_file:\n",
    "        for review_count, line in enumerate(review_text_file):\n",
    "            pass\n",
    "\n",
    "    print(u'Text from {:,} restaurant reviews written to the new txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process takes me about 2.5 minutes on my dual-core i5 laptop. Moreover, ther are a total of 2,927,731 reviews found, over three times as many as in 2016. I now have the IDs and review texts in separate files, and real parsing of the text can now begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Introduction to text processing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a means to perform NLP tasks on these reviews just collected. It is both free-to-use and incredibly efficient and fast, much moreso than NLTK. It has been designed not for research, but for commerical and industrial usage. Below, a sample review has been printed. With it, I can get a better idea of what kind of data I'm working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "with codecs.open(review_text_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing this entire thing with state-of-the-art NLP technology only took 23.2 milliseconds on my computer. Super fast!\n",
    "\n",
    "For listing all of the sentences in the sample review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing all of the named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing each word's POS tag:\n",
    "\n",
    "(We'll be using *pandas* for this for its easy displaying and further parsing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lemmas and shape analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "            columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each lemma is its token, minus inflections or tenses. Also note that all pronouns have been converted to *-PRON-*. (I can only guess this is because all pronouns can be see as identical information-wise, so there's no reason to save them as separate lemmas.)\n",
    "\n",
    "For listing tokens and their analytics (like what kind of named entity they are and where a token lies within a named entity phrase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "            columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For listing stopwords, punctuation, whitespace, numbers, unstandard vocab words, probability of appearance in the corpus, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                    token.prob,\n",
    "                    token.is_stop,\n",
    "                    token.is_punct,\n",
    "                    token.is_space,\n",
    "                    token.like_num,\n",
    "                    token.is_oov)\n",
    "                   for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                 columns=['text',\n",
    "                         'log_probability',\n",
    "                         'stop?',\n",
    "                         'punctuation?',\n",
    "                         'whitespace?',\n",
    "                         'number?',\n",
    "                         'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                      .applymap(lambda x: u'Yes' if x else u''))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Automatic Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with these tokens is that independently, they do not properly reflect the relationships between one another. For example, above, *do* and *n't* are considered separate tokens, but they would be better reflected as a single collocation since *n't* will never appear apart from *do* (or alternative modal verbs).\n",
    "\n",
    "From the *gensim* library, I will be able to complete phrase modeling over the current list of tokens to find such relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Because of Windows Anaconda, a warning appears without this filter.\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four steps that must be done in data preparation here:\n",
    "- a) Segment text of complete reviews into sentences and normalize text\n",
    "- b) First-order phrase modeling\n",
    "- c) Second-order phrase modeling\n",
    "- d) Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "#### a) Segment text of complete reviews into sentences & normalize text\n",
    "\n",
    "Three helper functions will be used for this step. They will, respectively, find punctuation and whitespace, line breaks, and word lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\" eliminates purely punctuation and whitespace tokens\"\"\"\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"reads in each review and unescapes the line-breaks\"\"\"\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"uses spaCy to parse reviews, lemmatize, and tokenize sentences\"\"\"\n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                 batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                            if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*lemmatized_sentence_corpus* loops over the original review text and outputs it to *unigram_sentences_all* one normalized sentence at a time, with one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_text_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process took me 2 hours and 48 minutes to complete on my modest computer.\n",
    "\n",
    "The Gensim library provides a LineSentence class that streams documents of this format from disk, allowing massive scaling for huge amounts of data that now does not need to be saved in whole into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these sentences look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 450, 460):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these new sentences are far simpler grammatically than they would have been before. This presents a glimpse at what the final outcome of this stage will look like in the end.\n",
    "\n",
    "#### b) First-order phrase modeling\n",
    "\n",
    "From these normalized sentences, one can now use *gensim.models.Phrases* to find two-word collocations and link them together into single tokens. \n",
    "\n",
    "For example, in one of the sentences above are the words \"soy sauce\". Though they are technically separate words, when together as a bigram they act as a distinct entity. Hence, by training a model on frequent bigrams, one can disambiguate cases like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This process took a little more than 14 minutes to complete on my computer. By saving the model to its filepath, it can be run on unrelated text. This process allows different models formed from different corpora to be cross-compared.\n",
    "\n",
    "I can now edit the original sentences to account for these bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                        'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a little more than 26 minutes on my computer. Let's take a look at what I'm left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what has been achieved? Looking at the outputted sentences above, now two-word phrases like \"deep fried\" and \"pot stickers\" are now written out as \"deep\\_fried\" and \"pot\\_stickers\". This gives a better representation of what words co-occur together at a rate far greater than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Second-order phrase modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have concatenated bigrams, but having trigrams accounted for as well might be a good idea. This involves nearly the same exact code found in part (b), but this time instead of running the new trigram model over the unigram sentences, I'll run it over the just-made bigram ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                     'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:  # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process took almost 18 minutes to complete on my computer. Again, I'm saving this trigram model in its own file so that it could be used again in the future on other corpora. Now, this second-order phrase model can be applied to the first-order transformed sentences, and the results of such will be outputted to a third sentences file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:  # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took almost 27 minutes on my computer. Now there is a file consisting of every sentence marked with trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 270, 280):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I do all this? Because now, instead of the phrase \"p.f. chang 's\" being represented as three separate words, it is now concatenated into \"p.f.\\_chang\\_'s\". This is now correctly marked as a single entity instead of as three.\n",
    "\n",
    "Technically, I could repeat this process indefinitely with higher and higher ngrams, but very little is gained information-wise after trigrams, so here I stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Apply text normalization and second-order phrase model to text of complete reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? I now have these useful models and lots of sentences, but I was originally looking at full reviews. Therefore, I can run the original texts through these models (and even remove imformation-poor stop-words to boot) and be left with a semantically-rich equivalent to our original text, ignoring most of grammar and focusing on the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                       'trigram_tranformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_review in nlp.pipe(line_review(review_text_filepath),\n",
    "                                    batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize, and remove punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "            \n",
    "            # apply the first- and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove stopwords, using SpaCy's nifty premade list\n",
    "            trigram_review = [term for term in trigram_review if term not in spacy.en.STOP_WORDS]\n",
    "            \n",
    "            # finally, output this to the new file, review by review as line by line\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a little more than 3 hours and 47 minutes to complete on my computer.\n",
    "\n",
    "Thus, to compare the text from before and after NLP transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'Original:\\n')\n",
    "for review in it.islice(line_review(review_text_filepath), 11, 12):\n",
    "    print(review)\n",
    "    \n",
    "print(u'----\\n')\n",
    "print(u'Transformed:\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious through a comparison of both of these that while a lot of content has been removed from the original text, the meat-and-potatoes remains. This information is far more useful for the semantic topic modelling that'll be applied next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualizing topic models with pyLDAvis\n",
    "\n",
    "*Topic Modeling* is a set of techniques where given a large corpus, inferences can be made between words such that many clusters of words appearing in similar contexts, or 'topics', can be determined and extracted. These topics can then be compared to the contexts found in specific documents, such that each document can be classified according to these contexts. In this sense, the computer is able to understand semantic connections between words without actually understanding the words themselves.\n",
    "\n",
    "Here, [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), or [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), is to be used. This approach was designed in 2003 and proves highly successful and easy to implement in Python (though there are of course other topic modeling techniques that too could be used as well).\n",
    "\n",
    "In LDA, documents are first analyzed not as massive strings, but merely as bags-of-words. A lexicon consisting of all the vocabulary found across all documents is created, then each document is described as a sparse matrix with the counts of each token found in each document. A massive and sparse *document-term matrix* results, and it is from here that analysis can be done.\n",
    "\n",
    "The *Latent* in LDA can be seen as synonymous to *hidden*. There are a number of hidden topics that the documents can be described by, and the total number of topics is determined by human inference. In the examples below, 50 topics are found, though more or less would yield denser but less accurate topics or sparser but more comprehensive topics, respectively.\n",
    "\n",
    "The [*Dirichlet*](https://en.wikipedia.org/wiki/Dirichlet_distribution) in LDA is a type of distribution discovered by Gustav Lejeune Dirichlet; this distribution is assumed to be the probability distribution that the documents, topics, and tokens jointly follow. Again, there are alternative options that could be utilized, but LDAs work quite well for instances like these.\n",
    "\n",
    "Finally, it is important to note that this is an instance of unsupervised learning. This means that the algorithm will discover these 50 topics on its own, with no previous knowledge of what each topic is. Because of this, I will have to label each topic myself in the end, as their true definitions will remain hidden to the computer and to me.\n",
    "\n",
    "Luckily, gensim again has modules that can be used for data processing and for LDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Because of Windows Anaconda, a warning appears without this filter.\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's *Dictionary* class allows the full lexicon of the Yelp review corpus to be modeled. The trigram dictionary found in the previous section will be used in forming the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "    \n",
    "    # iterate the reviews and build a dictionary of trigrams\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are too rare or common (so little information gain) from the dict\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    \n",
    "    # reassign integer ids removed from the filtering to conserve space\n",
    "    trigram_dictionary.compactify()\n",
    "    \n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "\n",
    "# load the finished dictionary back in after it's been built\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process only about 5 minutes on my computer. The LDA utilizes a bag-of-words model to represent all the words and their counts, and this is now saved as a sparse matrix with the dimensions of the number of reviews by the number of total vocab words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                   'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function that reads in a file and yields a bow representation from it\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    #generate the bow representation for the reviews and save as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                      trigram_bow_generator(trigram_reviews_filepath))\n",
    "\n",
    "# load the finished corpus back in after it's been built\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took about 9.5 minutes on my computer. I pass into the LDA model the sparse matrix made above, the number of topics to be found (here 50), and the total lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory,\n",
    "                                 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again.\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                          num_topics=50,\n",
    "                          id2word=trigram_dictionary,\n",
    "                          workers=1)\n",
    "        \n",
    "    lda.save(lda_model_filepath)\n",
    "\n",
    "# load the finished model back in after it's been built\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a little over 2 hours to complete. With this now-finished LDA model, each topic's most influential words can be seen, and from this one can infer roughly what each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    prints out a formatted list of the top terms of a given topic number\n",
    "    \"\"\"\n",
    "    \n",
    "    print(u'{:20} {}\\n'.format(u'term', u'frequency'))\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_topic(topic_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 15 contains words like *bbq*, *sauce*, *smoke*, and *brisket*, all of which relate to **BBQ**. A similar process of inference has been repeated for each of the topics, and these have been listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0: u'interior',\n",
    "              1: u'oriental',\n",
    "              2: u'breakfast',\n",
    "              3: u'service',\n",
    "              4: u'arrival',\n",
    "              5: u'cost',\n",
    "              6: u'nightlife',\n",
    "              7: u'alternative',\n",
    "              8: u'hip',\n",
    "              9: u'bar',\n",
    "              10: u'japanese',\n",
    "              11: u'pizza',\n",
    "              12: u'bistro',\n",
    "              13: u'steak',\n",
    "              14: u'overnight',\n",
    "              15: u'bbq',\n",
    "              16: u'family',\n",
    "              17: u'awesomesauce',\n",
    "              18: u'dessert',\n",
    "              19: u'selections',\n",
    "              20: u'mexican',\n",
    "              21: u'indian',\n",
    "              22: u'discount',\n",
    "              23: u'french',\n",
    "              24: u'lunch',\n",
    "              25: u'thai',\n",
    "              26: u'location',\n",
    "              27: u'parking/cajun/utensils',\n",
    "              28: u'terribad',\n",
    "              29: u'positive',\n",
    "              30: u'buffet',\n",
    "              31: u'tex-mex',\n",
    "              32: u'cajun',\n",
    "              33: u'burgers',\n",
    "              34: u'ramen',\n",
    "              35: u'fancy',\n",
    "              36: u'alcohol/south',\n",
    "              37: u'atmosphere',\n",
    "              38: u'crowdedness',\n",
    "              39: u'ice cream/delivery',\n",
    "              40: u'french',\n",
    "              41: u'management',\n",
    "              42: u'experience',\n",
    "              43: u'quality',\n",
    "              44: u'letters/numbers',\n",
    "              45: u'seafood',\n",
    "              46: u'upper-class',\n",
    "              47: u'celebration',\n",
    "              48: u'breakfast diner',\n",
    "              49: u'awesomesauce'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory,\n",
    "                                   'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'wb') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've appended all of these topic names to the original file. Although not all topics are easy to infer, each can be roughly associated with some distinct aspect of restaurants and cuisine.\n",
    "\n",
    "However, being able to visually inspect how each topic relates to one another in a graphical format would help to better understand just how each topic relates to one another. Luckily, pyLDAvis allows one to easily do such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory,\n",
    "                                   'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again\n",
    "    \n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda,\n",
    "                                              trigram_bow_corpus,\n",
    "                                              trigram_dictionary)\n",
    "    \n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the finished pyLDAvis data back in after it's been built       \n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pyLDAvis.display(pyLDAvisModel), a visualization can be displayed directly in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_text_filepath),\n",
    "                         review_number,\n",
    "                         review_number + 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    take the original text of a review and\n",
    "    (1) parse it using SpaCy\n",
    "    (2) pre-process the text\n",
    "    (3) create a bow representation\n",
    "    (4) create an LDA representation\n",
    "    (5) print a sorted list of the top topics from the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # (1) parse the text using SpaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # (2) pre-process the text\n",
    "    #  - lemmatize and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                     if not punct_space(token)]\n",
    "    \n",
    "    #  - apply the first- and second-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    #  - remove stopwords\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                     if not term in spacy.en.STOP_WORDS]\n",
    "    \n",
    "    # (3) create a bow representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # (4) create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly rated topics first\n",
    "    review_lda = sorted(review_lda, key=(lambda topic_number_freq: -topic_number_freq[1]))\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "        \n",
    "        # (5) print a sorted list of the top topics from the LDA representation\n",
    "        print('{:25} {}'.format(topic_names[topic_number],\n",
    "                               round(freq, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(50)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(100)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Word vector models with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again\n",
    "    \n",
    "    # initiate the model and perform training's first epoch\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                       min_count=20, sg=1, workers=2)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)\n",
    "    \n",
    "    # repeat another 11 epochs\n",
    "    for i in range(1,12):\n",
    "        food2vec.train(trigram_sentences,\n",
    "                      total_examples=food2vec.corpus_count,\n",
    "                      epochs=food2vec.iter)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model back in after it's been built\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print(u'{} training epochs so far.'.format(food2vec.train_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took over 15 hours total on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a list of the terms, integer indices, and term counts from food2vec's vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                for term, voc in food2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by term counts to order most frequent terms first\n",
    "ordered_vocab = sorted(ordered_vocab, key=(lambda term_index_count: -term_index_count[2]))\n",
    "\n",
    "# unzip the three into separate list\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a Pandas dataframe with the vectors as data and the terms as rows\n",
    "word_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n",
    "                           index=ordered_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    Look up the topn most similar terms to the token and print them as  a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "        print(u'{:20} {}'.format(word, round(similarity, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms(u\"mcdonald_'s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms(u'happy_hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms(u'pasta', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors of words in add and subtract,\n",
    "    and print the topn most similar terms to the combined vector\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = food2vec.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'breakfast', u'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'lunch', u'night'], subtract=[u'day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'taco', u'chinese'], subtract=[u'mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'bun', u'mexican'], subtract=[u'american'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'filet_mignon', u'seafood'], subtract=[u'beef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'coffee', u'snack'], subtract=[u'drink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"mcdonald_'s\", u'fine_dining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"denny_'s\", u'fine_dining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'italian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'pancakes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'pizza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'wine', u'barley'], subtract=[u'grapes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualizing word2vec with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_input = word_vectors.drop(spacy.en.STOP_WORDS, errors=u'ignore')\n",
    "tsne_input = tsne_input.head(5000)\n",
    "\n",
    "tsne_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_filepath = os.path.join(intermediate_directory,\n",
    "                            u'tsne_model')\n",
    "\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory,\n",
    "                                    u'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False: # Once this is run once it does not need to be run again\n",
    "    \n",
    "    tsne = TSNE()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    \n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "        \n",
    "    pd.np.save(tsne_vectors_filepath, tsne_vectors)\n",
    "\n",
    "    \n",
    "with open(tsne_filepath, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_vectors[u'word'] = tsne_vectors.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the dataframe as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# create a plot and configure the external variables\n",
    "tsne_plot = figure(title=u't-SNE Word Embeddings',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools = (u'pan, wheel_zoom, box_zoom',\n",
    "                            u'box_select, resize, reset'),\n",
    "                   active_scroll = u'wheel_zoom')\n",
    "\n",
    "# add a hovel tool to display words on mouse-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips = u'@word'))\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                size=10, hover_line_color=u'black')\n",
    "\n",
    "# configure the plot's visual elements\n",
    "tsne_plot.title.text_font_size = value(u'16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# the plot is complete! Let's look at it!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
